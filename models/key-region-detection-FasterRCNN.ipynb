{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torchvision\n",
    "\n",
    "from datasets import KeyboardBBoxDataset\n",
    "from datasets.util import *"
   ],
   "id": "bee690c24bd2416f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DATASET_VERSION = 2\n",
    "image_paths = get_dataset_paths(DATASET_VERSION)\n",
    "len(image_paths)"
   ],
   "id": "c2f3560e09951a8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mean, std = get_dataset_norm_params(DATASET_VERSION)\n",
    "mean, std"
   ],
   "id": "eaa4273e50d3d59b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_paths, test_paths, valid_paths = split_train_test_valid(image_paths, 0.8, 0.16)\n",
    "\n",
    "train_dataset = KeyboardBBoxDataset(train_paths)\n",
    "valid_dataset = KeyboardBBoxDataset(valid_paths)\n",
    "test_dataset = KeyboardBBoxDataset(test_paths)\n",
    "\n",
    "len(train_dataset), len(valid_dataset), len(test_dataset)"
   ],
   "id": "12fa6a19ab20c72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "train_dataset.set_transforms([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    # transforms.RandomAffine(degrees=30, shear=30, translate=(0.25, 0.25)),\n",
    "    # transforms.RandomPerspective(distortion_scale=0.25, p=0.5),\n",
    "    # transforms.RandomErasing(p=0.5),\n",
    "    transforms.ToPureTensor(),\n",
    "])\n",
    "\n",
    "train_dataset.set_augmentations([\n",
    "    transforms.RandomChannelPermutation(),\n",
    "    transforms.GaussianNoise(sigma=0.01),\n",
    "    transforms.RandomApply([transforms.GaussianNoise(sigma=0.01)], p=0.5),\n",
    "    transforms.Normalize(mean, std)\n",
    "])"
   ],
   "id": "73fbcd4949cac241",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "i = random.randint(0, len(train_dataset) - 1)\n",
    "train_dataset.show(i)\n",
    "train_dataset[i][1]"
   ],
   "id": "584c12bcbba14871",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "dl_kwargs = {\"batch_size\": BATCH_SIZE, \"num_workers\": 2, \"persistent_workers\": True, \"pin_memory\": True,\n",
    "             \"collate_fn\": zip_collate_fn}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, **dl_kwargs, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, **dl_kwargs)\n",
    "test_dataloader = DataLoader(test_dataset, **dl_kwargs)"
   ],
   "id": "ca5c1acd15c7d1d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "import lightning as pl\n",
    "from lightning.pytorch.callbacks.model_summary import summarize\n",
    "\n",
    "\n",
    "class KeyRegionFasterRCNN(pl.LightningModule):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        \n",
    "        # let's make the RPN generate 4 x 3 anchors per spatial\n",
    "        # location, with 5 different sizes and 3 different aspect\n",
    "        # ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "        # map could potentially have different sizes and\n",
    "        # aspect ratios\n",
    "        anchor_generator = AnchorGenerator(\n",
    "            sizes=((32, 64, 128, 256),),\n",
    "            aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "        )\n",
    "        \n",
    "        # let's define what are the feature maps that we will\n",
    "        # use to perform the region of interest cropping, as well as\n",
    "        # the size of the crop after rescaling.\n",
    "        # if your backbone returns a Tensor, featmap_names is expected to\n",
    "        # be [0]. More generally, the backbone should return an\n",
    "        # ``OrderedDict[Tensor]``, and in ``featmap_names`` you can choose which\n",
    "        # feature maps to use.\n",
    "        roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "            featmap_names=[\"0\"],\n",
    "            output_size=7,\n",
    "            sampling_ratio=2\n",
    "        )\n",
    "        \n",
    "        # put the pieces together inside a Faster-RCNN model\n",
    "        self.rcnn = FasterRCNN(\n",
    "            backbone,\n",
    "            num_classes=2,\n",
    "            rpn_anchor_generator=anchor_generator,\n",
    "            box_roi_pool=roi_pooler\n",
    "        )\n",
    "\n",
    "    def forward(self, *args):\n",
    "        return self.rcnn(args)\n",
    "\n",
    "    def _step(self, batch, stage):\n",
    "        if trainer.global_step == 0:\n",
    "            wandb.define_metric(f\"{stage}_loss\", summary=\"min\")\n",
    "\n",
    "        images, targets = batch\n",
    "        images = list(image for image in images)\n",
    "        targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "        self.rcnn.train()\n",
    "        loss_dict = self.rcnn(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        self.log(f\"{stage}_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._step(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=LEARNING_RATE)\n",
    "        # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=STEPS, eta_min=1e-5)\n",
    "        return {\"optimizer\": optimizer}  # \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}"
   ],
   "id": "66383c7635483051",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "wandb.login()"
   ],
   "id": "4ab3ae8ae8d8bce4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "wandb.finish()\n",
    "\n",
    "backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n",
    "# FasterRCNN needs the number of output channels in backbone.\n",
    "# For mobilenet_v2, it's 1280\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "model = KeyRegionFasterRCNN(backbone)\n",
    "model"
   ],
   "id": "660d32fb908f67e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "summarize(model)",
   "id": "e8e8f31c2e57987f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "EPOCHS = 40\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "PROFILE = False\n",
    "profiler = \"advanced\" if PROFILE else None\n",
    "\n",
    "logger = WandbLogger(project=\"mrover-keyboard-region-detection\", group=f\"MobileNetV2\")\n",
    "logger.experiment.config[\"dataset\"] = DATASET_VERSION\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=EPOCHS, log_every_n_steps=1, profiler=profiler, logger=logger, callbacks=[checkpoint_callback],)\n",
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader)"
   ],
   "id": "6144a084bfad5c08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.save(model.state_dict(), f\"models/binary_segmentation/{wandb.run.name}-FRCNN-MobileNetV2-v{DATASET_VERSION}.pt\")\n",
    "wandb.finish()"
   ],
   "id": "a84afc8707bbc5d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = KeyRegionDetectionModel()\n",
    "model.load_state_dict(\n",
    "    torch.load(f\"models/binary_segmentation/fresh-water-9-FRCNN-MobileNetV2-v{DATASET_VERSION}.pt\", weights_only=True))\n",
    "model.eval()"
   ],
   "id": "f9c039e125a0e7f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils import imshow\n",
    "import cv2\n",
    "\n",
    "img, _ = test_dataset[0]\n",
    "out = model(img)[0]\n",
    "img = reorder_image_axes(img.numpy()).copy()\n",
    "\n",
    "for quad in out[\"boxes\"]:\n",
    "    x1, y1, x2, y2 = map(int, quad.detach().numpy())\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), (1.0, 0, 0), 2)\n",
    "\n",
    "imshow(img)"
   ],
   "id": "d202cfad6c1c9fc5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
